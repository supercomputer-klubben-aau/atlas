@BEGINSKIP
This file generates:
   unsigned long fpuStress(unsigned long nrep, double *d);
*d is ignored right now.  
If nrep==0, then this function returns the number of flops per rep.
If nrep>0, then that many iterations are done, and return value is undefined.

This is file generates assembly that predicts peak FPU performance by doing
nothing except floating point multipy and accumulate instructions into
the specified number of accumulators.  

For x86, this basefile offers these user-settable variables:
   BASEFILE SETTINGS:
      KEY=ISA : SSE, AVX, AVXFMA (no x87 support)
      ivar=nmul: w/o FMA, this is number of regs to pipeline multiplies
      ivar=nacc: number of accumulators to pipeline runnings sums into
   CPP SETTINGS:
     SREAL,DREAL : choose single or double precision
     VLXX: XX can be 16,32,64: length (in bytes) of vectors

GENERAL NOTES:
 * Can detect total number of registers by increasing nacc until error,
   file uses nacc+nmul+1 registers
 * For x86, can detect length of vector registers by defining VL[16,32,64]

For ARM64, then ISA must be ARM64, Advanced SIMD will always be used,
and only nacc is referenced.

For ARM32, then we use only scalar FPU (not NEON), and only nacc is used.
@ENDSKIP
@ifdef ! nacc
   @abort nacc (# of accumulators) must be defined!
@endifdef
#include "atlas_asm.h"
@ISA AVXFMA AVX SSE
#ifdef SREAL
   @whiledef in vmovap movap xorp vxor vadd231p vaddp vmulp addp mulp
   #define @(in)d @(in)s
   @endwhile
#endif
@ISA AVX SSE
   @ifdef ! nmul
      @iexp nmul 1
   @endifdef
   @iif @iexp @(nmul) 1 ! @(nmul) @(nacc) ! &
      @abort nmul must be 1 or set to number of accumulators (@(nacc))
   @endiif
@ISA SSE
#define vmovapd movapd
@ISA AVXFMA
   @iexp nmul 0
@ISA AVXFMA AVX SSE
#if !defined(VL16) && !defined(VL32) && !defined(VL64)
   #error "Must set vector length in bytes by defining one of VL[16,32,64]!"
#endif

#define nrep %rdi
#define dp   %rsi
@multidef rn   ymm                 zmm     xmm
@whiledef ifs #else #elif@^defined(VL64) #ifdef@^VL16
@(ifs)
   @iexp ac 0
   @iwhile ac < nacc
   #define acc@(ac) %@(rn)@(ac)
      @iexp ac @(ac) 1 +
   @endiwhile
   #define zero %@(rn)@(ac)
   @iexp ml 0
   @iwhile ml < nmul
      @iexp ac @(ac) 1 +
   #define m@(ml)   %@(rn)@(ac)
      @iexp ml @(ml) 1 +
   @endiwhile
   @undef rn
@endwhile
#endif
/* 
 * rax                                    rdi        rsi
 * unsigned long fpuStress(unsigned long nrep, double *d);
 */
.text
.local LOOP
.local RET_LOOP_FLOPS
.global ATL_asmdecor(fpuStress)
ATL_asmdecor(fpuStress):
   cmp $0, nrep
   je RET_LOOP_FLOPS
   @ISA SSE `   xorpd zero, zero `
   @ISA AVX AVXFMA `   vxorpd zero, zero, zero`
@iexp ac 0
@iwhile ac < nacc
   vmovapd zero, acc@(ac)
   @iexp ac @(ac) 1 +
@endiwhile
@iexp ml 0
@iwhile ml < nmul
   vmovapd zero, m@(ml)
   @iexp ml @(ml) 1 +
@endiwhile
   LOOP:
   @iexp ac 0
   @iexp ml 0
   @iwhile ac < nacc
@ISA AVXFMA
      vfmadd231pd zero, zero, acc@(ac)
@ISA AVX
      vaddpd m@(ml), m@(ml), acc@(ac)
      vmulpd zero, zero, m@(ml)
@ISA SSE
      addpd m@(ml), acc@(ac)
      mulpd zero, m@(ml)
@ISA AVX SSE
      @iexp ml @(ml) 1 +
      @iif ml = nmul
         @iexp ml 0
      @endiif
@ISA AVXFMA AVX SSE
      @iexp ac @(ac) 1 +
   @endiwhile
      dec nrep
   jnz LOOP

   ret
RET_LOOP_FLOPS:
   mov $@(nacc)*2, %rax  /* (#acc)*(mul+add=2) */
   #if defined(VL16)     /* 2 doubles per vector */
      add %rax, %rax  /* VLEN=4 at least */
   #elif defined(VL32) /* 4 doubles per vector */
      shl $2, %rax
   #elif defined(VL64) /* 8 doubles per vector */
      shl $3, %rax
   #endif
   #ifdef SREAL
      add %rax, %rax   /* single precision has twice the flops */
   #endif
   ret
@ISA ARM64
#define Mjoin(pre, nam) my_join(pre, nam)
#define my_join(pre, nam) pre ## nam
#ifdef SREAL
   #define TP .4s
#else
   #define TP .2d
#endif
@iexp ac 0
@iwhile ac < nacc
   #define acc@(ac) Mjoin(v@(ac),TP)
   @iexp ac @(ac) 1 +
@endiwhile
@iexp nz @(ac)
   #define zero Mjoin(v@(ac),TP)
   #define nrep x0
#define FSZ 8*8  /* save all regs regardless of how many used */
/* 
 * x0                                     x0          x1
 * unsigned long fpuStress(unsigned long nrep, double *d);
 */
.text
.local LOOP
.local RET_LOOP_FLOPS
.globl ATL_asmdecor(fpuStress)
.type   ATL_asmdecor(fpuStress), %function
ATL_asmdecor(fpuStress):
   cmp x0, 0
   bLE RET_LOOP_FLOPS
   sub SP, SP, #FSZ
/*
 * Store all callee-saved fpregs so all calls pay same cost
 */
   @iexp ac 8
   @iexp off 0
   @iwhile ac < 15
      @iexp acp1 @(ac) 1 +
   stp d@(ac), d@(acp1), [SP, #@(off)]
      @iexp off @(off) 16 +
      @iexp ac @(ac) 2 +
   @endiwhile

   eor v@(nz).16b, v@(nz).16b, v@(nz).16b
   @iexp ac 0
   @iwhile ac < nacc
   eor v@(ac).16b, v@(ac).16b, v@(ac).16b
      @iexp ac @(ac) 1 +
   @endiwhile

   LOOP:
   @iexp ac 0
   @iwhile ac < nacc
      fmla acc@(ac), zero, zero
      @iexp ac @(ac) 1 +
   @endiwhile
      subs nrep, nrep, 1
   bne LOOP
/*
 * Restore callee-saved fpregs
 */
   @iexp ac 8
   @iexp off 0
   @iwhile ac < 15
      @iexp acp1 @(ac) 1 +
   ldp d@(ac), d@(acp1), [SP, #@(off)]
      @iexp off @(off) 16 +
      @iexp ac @(ac) 2 +
   @endiwhile
   add SP, SP, #FSZ
   ret
/*
 * Just return flop count for 1 loop iteration
 */
RET_LOOP_FLOPS:
   @iexp nf @(nacc) 2 * 2 *
   #ifdef SREAL
      mov x0, @(nacc)*2*4   /* (nacc)*(fmac)*(vlen) */
   #else
      mov x0, @(nacc)*2*2   /* (nacc)*(fmac)*(vlen) */
   #endif
   ret
.size ATL_asmdecor(fpuStress),.-ATL_asmdecor(fpuStress)
@ISA ARM32
#define Mjoin(pre, nam) my_join(pre, nam)
#define my_join(pre, nam) pre ## nam
#ifdef SREAL
   #define fcpyd fcpys
@iexp ac 0
@iwhile ac < nacc
   #define acc@(ac) s@(ac)
   @iexp ac @(ac) 1 +
@endiwhile
@iexp nz @(ac)
   #define zero s@(ac)
#else
@iexp ac 0
@iwhile ac < nacc
   #define acc@(ac) d@(ac)
   @iexp ac @(ac) 1 +
@endiwhile
@iexp nz @(ac)
   #define zero d@(ac)
#endif
   #define nrep r0
/* 
 * r0                                     r0          r1
 * unsigned long fpuStress(unsigned long nrep, double *d);
 */
.text
.local LOOP
.local RET_LOOP_FLOPS
.align 2
.globl ATL_asmdecor(fpuStress)
.type   ATL_asmdecor(fpuStress), %function
ATL_asmdecor(fpuStress):
   cmp r0, #0
   bLE RET_LOOP_FLOPS
   fstmDBd SP!, {d8-d15}  /* save all fp regs */

   eor r1, r1, r1
   #ifdef SREAL
      fmsr zero, r1
   #else
      fmdlr zero, r1
      fmdhr zero, r1
   #endif
   @iexp ac 0
   @iwhile ac < nacc
   fcpyd acc@(ac), zero
      @iexp ac @(ac) 1 +
   @endiwhile

   LOOP:
   @iexp ac 0
   @iwhile ac < nacc
      fmacd acc@(ac), zero, zero
      @iexp ac @(ac) 1 +
   @endiwhile
      subs nrep, nrep, #1
   bne LOOP

   fldmIad SP!, {d8-d15}  /* restore all fpregs */
   bx lr
/*
 * Just return flop count for 1 loop iteration
 */
RET_LOOP_FLOPS:
   mov r0, #@(nacc)*2   /* (nacc)*(fmac) */
   bx lr
.size ATL_asmdecor(fpuStress),.-ATL_asmdecor(fpuStress)
@ISA VSX POWER
#define Mjoin(pre, nam) my_join(pre, nam)
#define my_join(pre, nam) pre ## nam
@iexp ac 0
@iwhile ac < nacc
@ISA VSX   `   #define acc@(ac) v@(ac)`
@ISA POWER `   #define acc@(ac) f@(ac)`
   @iexp ac @(ac) 1 +
@endiwhile
@iexp nz @(ac)
@ISA VSX   `   #define zero v@(ac)`
@ISA POWER `   #define zero f@(ac)`
   #define nrep r3
@ISA POWER
#define FOFF 16           /* need space to store zero & caller's r1 */
#define FSIZE (FOFF+18*8)
@ISA VSX
#define ISZ 0        /* not save/restoring any iregs */
#define FSZ 6*8      /* saving f14-19, f20-31 overlapped with v regs */
#define VSZ 12*16    /* saving v[20-31] 16-byte vregs, overlapped wt f20-31 */
#define LSZ 0        /* don't need any local area */
#define MSZ 32       /* mandatory 32 bytes for stack frame tail */
#define VOFF (MSZ+LSZ)
#define FOFF (VOFF+VSZ)
#define IOFF (FOFF+FSZ)
#define FSIZE (MSZ+LSZ+VSZ+FSZ+ISZ)
@ISA POWER VSX
/* 
 * r3                                     r3          r4
 * unsigned long fpuStress(unsigned long nrep, double *d);
 */
.text
.local LOOP
.local RET_LOOP_FLOPS
.globl ATL_asmdecor(fpuStress)
.type   ATL_asmdecor(fpuStress), %function
ATL_asmdecor(fpuStress):
   cmpdi nrep, 0
   ble- RET_LOOP_FLOPS
   stdu r1, -FSIZE(r1)   /* allocate stack & store old stack ptr */
@ISA POWER
   #ifdef SREAL
      #define fmadd fmadds
   #endif
/*
 * Store callee-saved f14-f31
 */
   @iexp off 0
   @iexp ac 14
   @iwhile ac < 32
   stfd f@(ac), FOFF+@(off)(r1)
      @iexp ac @(ac) 1 +
      @iexp off @(off) 8 +
   @endiwhile
   xor r4, r4, r4
   std r4, 8(r1)
   lfd zero, 8(r1)
   @iexp ac 0
   @iwhile ac < nacc
      fmr acc@(ac), zero
      @iexp ac @(ac) 1 +
   @endiwhile
@ISA VSX
   std r2, 24(r1)        /* TOC reg always at 24(sp) */
/*
 * Store all callee-saved fpregs so all calls pay same cost: v20-v31,f14-f31
 * Need to save f[14-19], then f[20-31] handled by save of v of same #
 */
   addi r4, r1, VOFF
   @iexp ac 20
   @iwhile ac < 32
   stxvd2x v@(ac), r0, r4
   addi r4, r4, 16
      @iexp ac @(ac) 1 +
   @endiwhile
   @iexp off 0
   @iexp ac 14
   @iwhile ac < 20
   stfd f@(ac), FOFF+@(off)(r1)
      @iexp ac @(ac) 1 +
      @iexp off @(off) 8 +
   @endiwhile

   xxlxor zero, zero, zero
   @iexp ac 0
   @iwhile ac < nacc
      xxlxor acc@(ac), acc@(ac), acc@(ac)
      @iexp ac @(ac) 1 +
   @endiwhile

@ISA VSX POWER
   LOOP:
   @iexp ac 0
   @iwhile ac < nacc
@ISA VSX
      #if SREAL
         xvmaddadp acc@(ac), zero, zero, acc@(ac)
      #else
         xvmaddadp acc@(ac), zero, zero
      #endif
@ISA POWER
      fmadd acc@(ac), zero, zero, acc@(ac)
@ISA POWER VSX
      @iexp ac @(ac) 1 +
   @endiwhile
      addic. nrep, nrep, -1
   bne+ LOOP
/*
 * Restore callee-saved fpregs
 */
@ISA POWER
   @iexp off 0
   @iexp ac 14
   @iwhile ac < 32
   lfd  f@(ac), FOFF+@(off)(r1)
      @iexp ac @(ac) 1 +
      @iexp off @(off) 8 +
   @endiwhile
@ISA VSX
   ld r2, 24(r1)
   addi r4, r1, VOFF
   @iexp ac 20
   @iwhile ac < 32
   lxvd2x v@(ac), r0, r4
   addi r4, r4, 16
      @iexp ac @(ac) 1 +
   @endiwhile
   @iexp off 0
   @iexp ac 14
   @iwhile ac < 20
   lfd  f@(ac), FOFF+@(off)(r1)
      @iexp ac @(ac) 1 +
      @iexp off @(off) 8 +
   @endiwhile
@ISA POWER VSX
   addi r1, r1, FSIZE
   blr
/*
 * Just return flop count for 1 loop iteration
 */
RET_LOOP_FLOPS:
   #ifdef SREAL
      li x3, @(nacc)*2*4   /* (nacc)*(fmac)*(vlen) */
   #else
      li x3, @(nacc)*2*2   /* (nacc)*(fmac)*(vlen) */
   #endif
   blr
